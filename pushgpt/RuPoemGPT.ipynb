{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vkAHn7ZfVt9"
      },
      "source": [
        "# Building GPT on russian poetry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "JV-DSVmdmrWP"
      },
      "outputs": [],
      "source": [
        "# Make imports\n",
        "import torch\n",
        "import math\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gq8iHdtfGwt",
        "outputId": "4c6b6844-5d3c-4584-8c83-d74541009356"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-01-11 08:01:02--  https://github.com/gromdimon/ml-random/raw/master/pushgpt/data/data.txt\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/gromdimon/ml-random/master/pushgpt/data/data.txt [following]\n",
            "--2024-01-11 08:01:02--  https://raw.githubusercontent.com/gromdimon/ml-random/master/pushgpt/data/data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2650695 (2.5M) [text/plain]\n",
            "Saving to: ‘data.txt’\n",
            "\n",
            "data.txt            100%[===================>]   2.53M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-01-11 08:01:03 (39.1 MB/s) - ‘data.txt’ saved [2650695/2650695]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Prepare dataset\n",
        "!wget https://github.com/gromdimon/ml-random/raw/master/pushgpt/data/data.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV00UFSvgrRz",
        "outputId": "592c7ae9-3fd7-45b8-96df-2e80b85aac26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text: 1495941\n"
          ]
        }
      ],
      "source": [
        "with open('data.txt', 'r') as f:\n",
        "  text = f.read()\n",
        "\n",
        "print(f'Length of text: {len(text)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hR880vloitT2",
        "outputId": "ddd81060-5ceb-478f-a4b2-377a55b8b920"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ты никогда меня, Алина, не любила.\n",
            "Не виновата ты - я сам обманывал себя! \n",
            "Надежда тщетная мне льстила, \n",
            "Когда украдкою смотрел я на тебя! \n",
            "Ты говорила - мне казалось, \n",
            "Что в смысле слов твоих есть тайный смысл любви; \n",
            "Я думал так, когда в мечты твои\n",
            "Моя случайно прививалась; \n",
            "Так, словом: каждый ша\n"
          ]
        }
      ],
      "source": [
        "print(text[:300])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcc7E_U6hAbb",
        "outputId": "8ef92da2-ce72-4bb8-e6c6-55735519beaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\n",
            " !\"#$%'()*+,-./0123456789:;<=>?ABCDEFGHIJKLMNPQRSTUVY[\\]_abcdefghijklmnopqrstuvwxyz{}~¤«»́;АБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЫЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяёө‎–—―“„…﻿\n",
            "Vocabulary size: 165\n"
          ]
        }
      ],
      "source": [
        "# unique chars\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(f'Vocabulary size: {vocab_size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnSc4wGPjBR2",
        "outputId": "bf47058f-5330-4161-dfe0-93e73e8dc187"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100, 127, 139, 123, 125, 140, 141, 125, 142, 132, 13, 2, 135, 131, 139, 3]\n",
            "Здравствуй, мир!\n"
          ]
        }
      ],
      "source": [
        "# Encoding\n",
        "stoi = { ch:i for i, ch in enumerate(chars)}\n",
        "itos = { i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]   # encoder: take a string and return numerical values\n",
        "decode = lambda l: ''.join([itos[i] for i in l])   # decoder: take a list of integers and return a corresponding string\n",
        "\n",
        "print(encode(\"Здравствуй, мир!\"))\n",
        "print(decode(encode(\"Здравствуй, мир!\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVbtT47Fj4E3",
        "outputId": "b0b72062-ace3-41d1-f728-8aa3f1a68ce3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1495941]) torch.int64\n",
            "tensor([111, 150,   2, 136, 131, 133, 137, 126, 127, 123,   2, 135, 128, 136,\n",
            "        154,  13,   2,  93, 134, 131, 136, 123,  13,   2, 136, 128,   2, 134,\n",
            "        153, 124, 131, 134, 123,  15,   1, 106, 128,   2, 125, 131, 136, 137,\n",
            "        125, 123, 141, 123,   2, 141, 150,   2,  14,   2, 154,   2, 140, 123,\n",
            "        135,   2, 137, 124, 135, 123, 136, 150, 125, 123, 134,   2, 140, 128,\n",
            "        124, 154,   3,   2,   1, 106, 123, 127, 128, 129, 127, 123,   2, 141,\n",
            "        148, 128, 141, 136, 123, 154,   2, 135, 136, 128,   2, 134, 151, 140,\n",
            "        141, 131, 134, 123,  13,   2,   1, 103, 137, 126, 127, 123,   2, 142,\n",
            "        133, 139, 123, 127, 133, 137, 153,   2, 140, 135, 137, 141, 139, 128,\n",
            "        134,   2, 154,   2, 136, 123,   2, 141, 128, 124, 154,   3,   2,   1,\n",
            "        111, 150,   2, 126, 137, 125, 137, 139, 131, 134, 123,   2,  14,   2,\n",
            "        135, 136, 128,   2, 133, 123, 130, 123, 134, 137, 140, 151,  13,   2,\n",
            "          1, 116, 141, 137,   2, 125,   2, 140, 135, 150, 140, 134, 128,   2,\n",
            "        140, 134, 137, 125,   2, 141, 125, 137, 131, 144,   2, 128, 140, 141,\n",
            "        151,   2, 141, 123, 132, 136, 150, 132,   2, 140, 135, 150, 140, 134,\n",
            "          2, 134, 153, 124, 125, 131,  28,   2,   1, 122,   2, 127, 142, 135,\n",
            "        123, 134,   2, 141, 123, 133,  13,   2, 133, 137, 126, 127, 123,   2,\n",
            "        125,   2, 135, 128, 146, 141, 150,   2, 141, 125, 137, 131,   1, 105,\n",
            "        137, 154,   2, 140, 134, 142, 146, 123, 132, 136, 137,   2, 138, 139,\n",
            "        131, 125, 131, 125, 123, 134, 123, 140, 151,  28,   2,   1, 111, 123,\n",
            "        133,  13,   2, 140, 134, 137, 125, 137, 135,  27,   2, 133, 123, 129,\n",
            "        127, 150, 132,   2, 147, 123, 126,  13,   2, 131,   2, 133, 123, 129,\n",
            "        127, 150, 132,   2, 130, 125, 142, 133,   2, 139, 128, 146, 128, 132,\n",
            "         13,   2,   1, 106, 128, 125, 131, 136, 136, 150, 128,   2, 125,   2,\n",
            "        125, 131, 136, 128,   2, 135, 137, 128, 132,  13,   2,   1, 122,   2,\n",
            "        137, 124, 139, 123, 148, 123, 134,   2, 133,   2, 141, 128, 124, 128,\n",
            "         13,   2, 131, 144,   2, 134, 137, 129, 136, 137,   2, 138, 137, 136,\n",
            "        131, 135, 123, 154,  27,   2,   1, 108, 139, 131, 140, 125, 123, 131,\n",
            "        125, 123, 134,   2, 140, 128, 124, 128,   2, 125, 140, 128,  13,   2,\n",
            "        124, 150, 134, 137,   2, 146, 141, 137,   2, 146, 142, 129, 131, 135,\n",
            "          2,   1, 101,   2, 137, 141, 136, 137, 140, 131, 134, 137, 140, 154,\n",
            "          2, 133,   2, 127, 139, 142, 126, 131, 135,  13,   2,   1, 105, 136,\n",
            "        128,   2, 141, 137, 134, 151, 133, 137,   2, 140, 134, 123, 124, 142,\n",
            "        153,   2, 136, 123, 127, 128, 129, 127, 142,   2, 137, 140, 141, 123,\n",
            "        125, 134, 154, 154,  15,   2,   1, 106, 128,   2, 125, 131, 136, 137,\n",
            "        125, 123, 141, 123,   2, 141, 150,   3,   2, 106, 128, 141,  13,   2,\n",
            "        154,   2, 137, 127, 131, 136,   2, 125, 131, 136, 137, 132,   3,   2,\n",
            "          1, 106, 137,   2, 133, 123, 133,   2, 129, 128, 140, 141, 137, 133,\n",
            "        137,   2, 136, 123, 133, 123, 130, 123, 136, 151, 128,   3,  15,  15,\n",
            "          2,   1, 110, 135, 137, 141, 139, 131,  13,   2, 146, 141, 137,   2,\n",
            "        146, 142, 125, 140, 141, 125, 142, 153,  28,   2, 125, 140, 135, 137,\n",
            "        141, 139, 131, 140, 154,  13,   2, 146, 141, 137,   2, 140, 137,   2,\n",
            "        135, 136, 137, 132,  13,   2,   1, 111, 137, 126, 127, 123,  13,   2,\n",
            "        133, 123, 133,   2, 154,   2, 137, 127, 131, 136,  28,   2, 141, 137,\n",
            "        126, 127, 123,  13,   2, 133, 123, 133,   2, 154,   2, 140,   2, 141,\n",
            "        137, 124, 137, 132,  13,   2,   1,  95,   2, 139, 123, 130, 134, 142,\n",
            "        133, 128,   2, 134, 151,  13,   2, 131, 134, 131,   2, 138, 139, 131,\n",
            "          2, 140, 125, 131, 127, 123, 136, 151, 128,  13,   2,   1,  95, 140,\n",
            "        128, 126, 127, 123,   2,  14,   2, 141, 150,  28,   2, 141, 150,   2,\n",
            "        137, 127, 136, 123,   2, 125, 140, 128, 144,   2, 146, 142, 125, 140,\n",
            "        141, 125,   2, 135, 137, 131, 144,   2, 127, 142, 147, 137, 132,   3,\n",
            "          2,   1, 122,   2, 135, 142, 146, 142, 140, 151,   2, 139, 128, 125,\n",
            "        136, 137, 140, 141, 151, 153,  13,   2, 134, 153, 124, 137, 125, 151,\n",
            "        153,  13,   2,   1, 114, 137, 146, 142,   2, 130, 123, 124, 125, 128,\n",
            "        136, 131, 128, 135,   2, 130, 123,   2, 136, 128, 125, 136, 131, 135,\n",
            "        123, 136, 151, 128,   2, 135, 140, 141, 131, 141, 151,  28,   2,   1,\n",
            "        114, 137, 146, 142,   2, 125,   2, 141, 125, 137, 128, 132,   2, 133,\n",
            "        139, 137, 125, 131,   2, 140, 125, 137, 132,   2, 140, 141, 139, 123,\n",
            "        147, 136, 150, 132,   2, 140, 141, 150, 127,   2, 137, 135, 150, 141,\n",
            "        151,  28,   2,   1, 114, 137, 146, 142,   2, 140, 125, 137, 128, 132,\n",
            "          2, 137, 135, 150, 141, 151, 140, 154,   2, 133, 139, 137, 125, 151,\n",
            "        153,   2,   1, 101,  15,  15,  15,   2, 107,  13,   2, 133, 123, 133,\n",
            "          2, 141, 154, 129, 128, 134, 137,   2, 134, 153, 124, 131, 141, 151,\n",
            "          3,  15,  15,   1, 111, 150,   2, 144, 137, 146, 128, 147, 151,   2,\n",
            "        130, 136, 123, 141, 151,  13,   2, 135, 137, 132,   2, 127, 139, 142,\n",
            "        126,   3,   2, 133, 123, 133,   2, 127, 136, 131,   2, 135, 137, 131,\n",
            "          2, 141, 128, 133, 142, 141,  32,   1,  94, 134, 123, 126, 137, 127,\n",
            "        123, 139, 153,   2, 141, 128, 124, 154,  13,   2, 146, 141, 137,   2,\n",
            "        141, 150,   2, 125,   2, 140, 141, 139, 123, 136, 128,   2, 127, 123,\n",
            "        134, 128, 133, 137, 132,   1,  94, 128, 139, 128, 147, 151,   2, 142,\n",
            "        146, 123, 140, 141, 131, 128,   2, 125,   2, 135, 137, 128, 132,   2,\n",
            "        140, 142, 127, 151, 124, 128])\n"
          ]
        }
      ],
      "source": [
        "# Encode entire text and store as torch.Tensor\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fOjB_lc7kA8x"
      },
      "outputs": [],
      "source": [
        "# Split data for training and validation\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1THnl3Akf1U",
        "outputId": "c41f1aab-f8ef-4d35-9aec-baadebf00b02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([111, 150,   2, 136, 131, 133, 137, 126, 127, 123,   2])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "block_size = 10\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvVaYcpGkmXh",
        "outputId": "79fcefda-d91c-4538-8d6c-bb6854bef656"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "when input is tensor([111]) the target: 150\n",
            "when input is tensor([111, 150]) the target: 2\n",
            "when input is tensor([111, 150,   2]) the target: 136\n",
            "when input is tensor([111, 150,   2, 136]) the target: 131\n",
            "when input is tensor([111, 150,   2, 136, 131]) the target: 133\n",
            "when input is tensor([111, 150,   2, 136, 131, 133]) the target: 137\n",
            "when input is tensor([111, 150,   2, 136, 131, 133, 137]) the target: 126\n",
            "when input is tensor([111, 150,   2, 136, 131, 133, 137, 126]) the target: 127\n",
            "when input is tensor([111, 150,   2, 136, 131, 133, 137, 126, 127]) the target: 123\n",
            "when input is tensor([111, 150,   2, 136, 131, 133, 137, 126, 127, 123]) the target: 2\n"
          ]
        }
      ],
      "source": [
        "# Representation of what transformer internally should do\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCnUauk5lP5e",
        "outputId": "ad316316-38c3-4cb7-f9e7-d8119749ac7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1425684, 1321777,  839704,  852361])\n",
            "tensor([[134, 131,   2, 128, 126, 137,   1,  95, 139, 123],\n",
            "        [138, 128, 146, 123, 134, 151,   2, 140, 125, 137],\n",
            "        [134, 123, 141, 151,  32,   2, 146, 141, 137,   2],\n",
            "        [ 27,   2,  95, 128, 139, 128, 136, 131, 145, 123]])\n",
            "tensor([[131,   2, 128, 126, 137,   1,  95, 139, 123, 129],\n",
            "        [128, 146, 123, 134, 151,   2, 140, 125, 137, 153],\n",
            "        [123, 141, 151,  32,   2, 146, 141, 137,   2, 135],\n",
            "        [  2,  95, 128, 139, 128, 136, 131, 145, 123,   1]])\n"
          ]
        }
      ],
      "source": [
        "ix = torch.randint(len(data) - 10, (4,))\n",
        "print(ix)\n",
        "x = torch.stack([data[i:i+10] for i in ix])\n",
        "y = torch.stack([data[i+1:i+10+1] for i in ix])\n",
        "print(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3O45pM3DkywE",
        "outputId": "023b23a5-c962-416b-b453-9e8fef8a24f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "torch.Size([4, 10])\n",
            "tensor([[128, 124, 154,   2, 141, 139, 128, 125, 137, 129],\n",
            "        [139, 137, 127, 128, 141, 128, 134, 151,  27,   1],\n",
            "        [141, 131,   2, 135, 139, 123, 146, 136, 137, 132],\n",
            "        [138, 137, 127, 139, 142, 126,  32,   1, 106, 128]])\n",
            "targets:\n",
            "torch.Size([4, 10])\n",
            "tensor([[124, 154,   2, 141, 139, 128, 125, 137, 129, 131],\n",
            "        [137, 127, 128, 141, 128, 134, 151,  27,   1, 112],\n",
            "        [131,   2, 135, 139, 123, 146, 136, 137, 132,   2],\n",
            "        [137, 127, 139, 142, 126,  32,   1, 106, 128,   2]])\n",
            "----\n",
            "when input is [128] the target: 124\n",
            "when input is [128, 124] the target: 154\n",
            "when input is [128, 124, 154] the target: 2\n",
            "when input is [128, 124, 154, 2] the target: 141\n",
            "when input is [128, 124, 154, 2, 141] the target: 139\n",
            "when input is [128, 124, 154, 2, 141, 139] the target: 128\n",
            "when input is [128, 124, 154, 2, 141, 139, 128] the target: 125\n",
            "when input is [128, 124, 154, 2, 141, 139, 128, 125] the target: 137\n",
            "when input is [128, 124, 154, 2, 141, 139, 128, 125, 137] the target: 129\n",
            "when input is [128, 124, 154, 2, 141, 139, 128, 125, 137, 129] the target: 131\n",
            "when input is [139] the target: 137\n",
            "when input is [139, 137] the target: 127\n",
            "when input is [139, 137, 127] the target: 128\n",
            "when input is [139, 137, 127, 128] the target: 141\n",
            "when input is [139, 137, 127, 128, 141] the target: 128\n",
            "when input is [139, 137, 127, 128, 141, 128] the target: 134\n",
            "when input is [139, 137, 127, 128, 141, 128, 134] the target: 151\n",
            "when input is [139, 137, 127, 128, 141, 128, 134, 151] the target: 27\n",
            "when input is [139, 137, 127, 128, 141, 128, 134, 151, 27] the target: 1\n",
            "when input is [139, 137, 127, 128, 141, 128, 134, 151, 27, 1] the target: 112\n",
            "when input is [141] the target: 131\n",
            "when input is [141, 131] the target: 2\n",
            "when input is [141, 131, 2] the target: 135\n",
            "when input is [141, 131, 2, 135] the target: 139\n",
            "when input is [141, 131, 2, 135, 139] the target: 123\n",
            "when input is [141, 131, 2, 135, 139, 123] the target: 146\n",
            "when input is [141, 131, 2, 135, 139, 123, 146] the target: 136\n",
            "when input is [141, 131, 2, 135, 139, 123, 146, 136] the target: 137\n",
            "when input is [141, 131, 2, 135, 139, 123, 146, 136, 137] the target: 132\n",
            "when input is [141, 131, 2, 135, 139, 123, 146, 136, 137, 132] the target: 2\n",
            "when input is [138] the target: 137\n",
            "when input is [138, 137] the target: 127\n",
            "when input is [138, 137, 127] the target: 139\n",
            "when input is [138, 137, 127, 139] the target: 142\n",
            "when input is [138, 137, 127, 139, 142] the target: 126\n",
            "when input is [138, 137, 127, 139, 142, 126] the target: 32\n",
            "when input is [138, 137, 127, 139, 142, 126, 32] the target: 1\n",
            "when input is [138, 137, 127, 139, 142, 126, 32, 1] the target: 106\n",
            "when input is [138, 137, 127, 139, 142, 126, 32, 1, 106] the target: 128\n",
            "when input is [138, 137, 127, 139, 142, 126, 32, 1, 106, 128] the target: 2\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(142)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 10 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YbbKhqLnwOp"
      },
      "source": [
        "## Bigram Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HTFWWWYmhNo",
        "outputId": "e1b98e7c-0369-4da4-a2f2-bbd031e99399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([40, 165])\n",
            "tensor(5.5798, grad_fn=<NllLossBackward0>)\n",
            "\tуg-ГоъиgCсёT“gЦ\tеAохт\tмОШрөТm4fjИ‎Р—0Q!;¤дй «т‎оP<щm\\Н8{7–бc0zlфR»gё\"8к xAис)Лдь,ЯРор—6y_Фыж1m\\lФыy7\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(142)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "uZ85YZV7m6hE"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZVlL3w1m80u",
        "outputId": "df0f5264-c2d0-42d7-bc22-8b9876bc0ff7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.1023149490356445\n",
            "Expected random nll: 5.10594547390058\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n",
        "print(f'Expected random nll: {-math.log(1/vocab_size)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vPjfP2enkmz",
        "outputId": "e87902d6-2223-40cf-f73d-b23905c3db53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tТbЖЕъgsHoо9?oе2n4мo,яrц!QpP)6JjS{ХeраЙ0\t?g7со9ЗFNcИС…Г<аэ0кaУЛs=LKЭАf86E}a)П,¤kГпъfG¤ВМ*8hАшк—ёNАнвh*ЖЧ―Pd»\t%У}ялPbЖСд6к84Я1LюAмh(Uвi*ёДоr;Ы.б3xKыИз8Хвлm~л_>8ЯгШОУ\n",
            "\t„oVEwВЩВP)иAй9GРC+црbPLTэe<gЦХe„ihЧ?Йд―bмзрЛВ7Зl(Д8о“сBТp#teэ„Йиьи;\n",
            "—ЩyLМaq_R,БGө в5т0,;ЧuфtВСуЩt―;Yй  vl﻿́2k0‎ЗL>b-lx\n",
            "MF_{aBstPsЦCЮnяNА«НhР‎уЗw вCS‎8В E“—ьM3)\t7.―в'=ХJгR…\"Щ}KM)<щйC+BУ+у-2\n",
            "\tг\n",
            "–5N?;')–joщеCбf;s=6e]А«+-\\;.BТ{<m#ыjYВ/7ЭGЩ﻿\tб-bюөGnUikQCQхoщйЕ?йгvТ„о:ыЫЮ-—…ёъRJj+bрФ!7тr:33KrЦlЮ9НОCоSвй.2Ду;PH/ЭA;n~HнФжю\tГ﻿;S_ёщг_КqөгH~т?р\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f09HEI8nnlI"
      },
      "source": [
        "## Self-attention construction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJRZ9-sWnqXg",
        "outputId": "49d282f5-1212-4054-a98c-dd3e2b9c5363"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ],
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paG3iCV2n5TG",
        "outputId": "8bbefe0d-22b6-4a22-885e-44a5f7068698"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 10, 2])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(142)\n",
        "B,T,C = 4,10,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "rqi54T5Nn_SD"
      },
      "outputs": [],
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J1xxwA0oCrE",
        "outputId": "e79c2751-bce9-49a6-d5f8-959f906ad2f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceHjD0KvoE5a",
        "outputId": "8ffd361c-f60e-45d8-c2cb-c90bd999b237"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3sb1JKcoH_Q",
        "outputId": "7ff040b8-ad3b-486a-e1e0-f5de0b8602d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 10, 16])"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(142)\n",
        "B,T,C = 4,10,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSEfDw-XoOFI",
        "outputId": "83895a1e-7b13-4c05-9cfe-1e1d7588e6c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.4103, 0.5897, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.3415, 0.6370, 0.0215, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.1249, 0.0940, 0.1776, 0.6035, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0870, 0.1354, 0.6032, 0.1005, 0.0739, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0431, 0.0233, 0.7775, 0.0847, 0.0605, 0.0109, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0144, 0.0109, 0.0040, 0.0890, 0.8487, 0.0304, 0.0026, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0647, 0.0102, 0.5750, 0.0363, 0.0015, 0.0026, 0.3072, 0.0024, 0.0000,\n",
              "         0.0000],\n",
              "        [0.1009, 0.3238, 0.0078, 0.0294, 0.0263, 0.3270, 0.1144, 0.0251, 0.0453,\n",
              "         0.0000],\n",
              "        [0.0090, 0.0381, 0.0023, 0.0043, 0.1291, 0.0902, 0.0045, 0.1382, 0.5279,\n",
              "         0.0564]], grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw-v-8saoPho",
        "outputId": "ac6008f5-386b-4536-8c83-6dd9e7a690e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(142)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcqiiNU9ogP-",
        "outputId": "5c3cf550-f87b-4f1a-d461-e7dc9304b160"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.0734), tensor(1.0049))"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25hETe6doi0X",
        "outputId": "80952173-927d-4dcb-8266-b8f3e177dd18"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(6.5565e-09), tensor(1.0000))"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaxZj_cdom7M"
      },
      "source": [
        "## Actual Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mDa6juSopd-",
        "outputId": "a7166afb-4d3c-4148-cd6d-aab43e18d56b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.423845 M parameters\n",
            "step 0: train loss 5.2048, val loss 5.2012\n",
            "step 100: train loss 2.8167, val loss 2.7798\n",
            "step 200: train loss 2.6767, val loss 2.6694\n",
            "step 300: train loss 2.6230, val loss 2.6166\n",
            "step 400: train loss 2.5753, val loss 2.5711\n",
            "step 500: train loss 2.5177, val loss 2.5023\n",
            "step 600: train loss 2.4548, val loss 2.4574\n"
          ]
        }
      ],
      "source": [
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 64 # what is the maximum context length for predictions?\n",
        "max_iters = 8000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 8\n",
        "n_layer = 8\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6qwAosuspMH"
      },
      "source": [
        "### Results:\n",
        "1 - batch_size=16, block_size=32, max_iters=5000, n_embd=32, n_head=4, n_layer=4\n",
        "\n",
        "2 - batch_size=32, block_size=64, max_iters=8000, n_embd=64, n_head=8, n_layer=8\n",
        "\n",
        "| 1 | 2 |\n",
        "| --- | --- |\n",
        "| step 0: train loss 5.2444, val loss 5.2621 | step 0: train loss 5.2048, val loss 5.2012 |\n",
        "| step 100: train loss 2.9265, val loss 2.8814 | step 100: train loss 2.8167, val loss 2.7798 |\n",
        "| step 4999: train loss 2.0749, val loss 2.0706 | --- |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuDa_LtIpban",
        "outputId": "293c04f8-0872-4ec8-fa86-39fcafd8ceb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\" А исчах ночесился,\n",
            "И рекаетой ялося.У\n",
            "\n",
            "Прогверенальная домнего ужастный:\n",
            "Моловыхнуюга\n",
            "Скоры, душал дить?\n",
            "Во зей гна всетсина тошной\n",
            "Не мотерчистом ян на дорушать,\n",
            "Вольмом тучески мне \n",
            "Собшнылой почтетн,\n",
            "Не всюди во скруг и меня гманьстыхать голмок?\n",
            "\n",
            "От сfуть глюбу: и бурий поны\n",
            "Кров, тоборилсева родали путы ты буйном,\n",
            "Путный одешь, нильный бль, носль кела\n",
            "Замой к себет кракалый. \n",
            "Увет:\n",
            "И я покойть не на разликоговой —\n",
            "Ны бирок, гтоб приземраживнием бемчись в —\n",
            "Тамого слеuчистно сон погучи\n",
            "Скрось леннуться вот каторв,\n",
            "Вердце учем, суде стрыйну пучнала,\n",
            "Я крыласка был тих звы,\n",
            "Ракею! ровся дру можится,\n",
            "Цваю милый - блажен \n",
            "И ―ежный - прозыл\n",
            "Покого волныз обчал на,\n",
            "Раз дику…И хорилый, свершные койдети?\n",
            "Гот, милый нестрыта ндадлии?\n",
            "И замило когройки корькая;\n",
            "Старь в ввиды встрать прищари - — голлость пренян,\n",
            "И как слебой крыты слачй на покопыть,\n",
            "С отмой илот.\n",
            "\n",
            "Под свозыт гое - наей.\n",
            "\n",
            "Вот круг нечных за севей Керрануть -\n",
            "Бузненно вермыв вигу,\n",
            "Эточелося пригоробь!\n",
            "Вхорга весоранна в на угралыша,\n",
            "В нив казали мне меня след,\n",
            "С мотыхохи стой для взой\n",
            " мне десты, ни в слад! --\n",
            "Меж, хорял Фрей стицет,\n",
            "Как намко разой миня и от полецаной\n",
            "Гдается ли онбнуть — и безна жизнь буденький;\n",
            "\"Где корком новный друг\n",
            "Поей,\n",
            "В крастью пучтих\n",
            "Теня и вего него жслом,\n",
            "Дла восторг, меж,\n",
            "Послитыми мне души -\n",
            "Риз тишилое владост полнца,\n",
            "В село прадвой мне сворых - гора ждет ты верду нествоченствой,\n",
            "И прильне в был, зак мтинала---\n",
            "На мирны видем, копеченых\n",
            "\tРузял блубкай,\n",
            "Мне матерьемичало к недлико,\n",
            "Напоресть, расце денит.\n",
            "Пей имет: не суждянье конщий смоличко!\n",
            "А тепй покранный, вы здепно дол-коно\n",
            "Настанбыл в трепо прорла вест невучкой,\n",
            "Лю щетаи пмолною в не гов?\n",
            "В каже строть днгу выде слеком..\n",
            "Кольных мне во под,\n",
            "‎\tМоу?....\n",
            "Уже нам претнокой пелесальной.\n",
            "Вета вежно увимали ляней в ботрн в ей им в Что м </i>\n",
            "Знывийство бытятся,\n",
            "С закалося ли вдруг и мралася,\n",
            "От нитто бль. исть - кола плавь и любили!\n",
            "\n",
            "Венныйл молой, пмечтую и с покеьном спотретанила\n",
            "\tВ треко стенной не несчуго многа,\n",
            "Иль бнезре\n"
          ]
        }
      ],
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8Dd6Y_kipZwd",
        "outputId": "99c0add1-0f09-4053-d651-050d9a1ca5a4"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_c88789b8-1ea6-41e0-8b92-412b64a17be5\", \"checkpoint.pt\", 995692)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!mkdir model\n",
        "torch.save(m.state_dict(), 'model/checkpoint.pt')\n",
        "\n",
        "# download checkpoint file\n",
        "files.download('model/checkpoint.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oInzVVyMp5mm",
        "outputId": "a959002c-deaa-4b8b-ccbd-9106a41ce159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "odict_keys(['token_embedding_table.weight', 'position_embedding_table.weight', 'blocks.0.sa.heads.0.tril', 'blocks.0.sa.heads.0.key.weight', 'blocks.0.sa.heads.0.query.weight', 'blocks.0.sa.heads.0.value.weight', 'blocks.0.sa.heads.1.tril', 'blocks.0.sa.heads.1.key.weight', 'blocks.0.sa.heads.1.query.weight', 'blocks.0.sa.heads.1.value.weight', 'blocks.0.sa.heads.2.tril', 'blocks.0.sa.heads.2.key.weight', 'blocks.0.sa.heads.2.query.weight', 'blocks.0.sa.heads.2.value.weight', 'blocks.0.sa.heads.3.tril', 'blocks.0.sa.heads.3.key.weight', 'blocks.0.sa.heads.3.query.weight', 'blocks.0.sa.heads.3.value.weight', 'blocks.0.sa.proj.weight', 'blocks.0.sa.proj.bias', 'blocks.0.ffwd.net.0.weight', 'blocks.0.ffwd.net.0.bias', 'blocks.0.ffwd.net.2.weight', 'blocks.0.ffwd.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.sa.heads.0.tril', 'blocks.1.sa.heads.0.key.weight', 'blocks.1.sa.heads.0.query.weight', 'blocks.1.sa.heads.0.value.weight', 'blocks.1.sa.heads.1.tril', 'blocks.1.sa.heads.1.key.weight', 'blocks.1.sa.heads.1.query.weight', 'blocks.1.sa.heads.1.value.weight', 'blocks.1.sa.heads.2.tril', 'blocks.1.sa.heads.2.key.weight', 'blocks.1.sa.heads.2.query.weight', 'blocks.1.sa.heads.2.value.weight', 'blocks.1.sa.heads.3.tril', 'blocks.1.sa.heads.3.key.weight', 'blocks.1.sa.heads.3.query.weight', 'blocks.1.sa.heads.3.value.weight', 'blocks.1.sa.proj.weight', 'blocks.1.sa.proj.bias', 'blocks.1.ffwd.net.0.weight', 'blocks.1.ffwd.net.0.bias', 'blocks.1.ffwd.net.2.weight', 'blocks.1.ffwd.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.sa.heads.0.tril', 'blocks.2.sa.heads.0.key.weight', 'blocks.2.sa.heads.0.query.weight', 'blocks.2.sa.heads.0.value.weight', 'blocks.2.sa.heads.1.tril', 'blocks.2.sa.heads.1.key.weight', 'blocks.2.sa.heads.1.query.weight', 'blocks.2.sa.heads.1.value.weight', 'blocks.2.sa.heads.2.tril', 'blocks.2.sa.heads.2.key.weight', 'blocks.2.sa.heads.2.query.weight', 'blocks.2.sa.heads.2.value.weight', 'blocks.2.sa.heads.3.tril', 'blocks.2.sa.heads.3.key.weight', 'blocks.2.sa.heads.3.query.weight', 'blocks.2.sa.heads.3.value.weight', 'blocks.2.sa.proj.weight', 'blocks.2.sa.proj.bias', 'blocks.2.ffwd.net.0.weight', 'blocks.2.ffwd.net.0.bias', 'blocks.2.ffwd.net.2.weight', 'blocks.2.ffwd.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.sa.heads.0.tril', 'blocks.3.sa.heads.0.key.weight', 'blocks.3.sa.heads.0.query.weight', 'blocks.3.sa.heads.0.value.weight', 'blocks.3.sa.heads.1.tril', 'blocks.3.sa.heads.1.key.weight', 'blocks.3.sa.heads.1.query.weight', 'blocks.3.sa.heads.1.value.weight', 'blocks.3.sa.heads.2.tril', 'blocks.3.sa.heads.2.key.weight', 'blocks.3.sa.heads.2.query.weight', 'blocks.3.sa.heads.2.value.weight', 'blocks.3.sa.heads.3.tril', 'blocks.3.sa.heads.3.key.weight', 'blocks.3.sa.heads.3.query.weight', 'blocks.3.sa.heads.3.value.weight', 'blocks.3.sa.proj.weight', 'blocks.3.sa.proj.bias', 'blocks.3.ffwd.net.0.weight', 'blocks.3.ffwd.net.0.bias', 'blocks.3.ffwd.net.2.weight', 'blocks.3.ffwd.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'ln_f.weight', 'ln_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "\tНи его;\n",
            "Неь вывное слышов молю!\n",
            "Нелит призашных утесть слодал,\n",
            "Поднянал,\n",
            "Я тольнулоя моея,\n",
            "Он и бега-то издашною\n",
            "Г, разверн, рачилыхо крглинц!\n",
            "Мой Анет печерт, крощие!\n",
            "На приядь прожыть тихой! открушки,\n",
            "Качалесь в бе нимая снопоть;\n",
            "Нет мне блежно ли недили. \n",
            "От........., — найдушк здесь и щно\n",
            "\tЗдяли, на стра\n",
            "Мотубкой звлать многовравный бравы,\n",
            "Носты яркый седил - мняют обывит.\n",
            "Колных зиет; как согуду знатвенний;\n",
            "Брай, пасница Мне пасч голен.\n",
            "Несе добрытый мгутицо,\n",
            " роз скатеи к пойтам отрему помоле унбный!.. \n",
            "Как спок сир. пути сих,\n",
            "Кустой пойный тамный днейи,\n",
            "Взящулся прягаля и бужниха;\n",
            "Можкальный житатье\n",
            "Люрым с разли,\n",
            "Сбреза вношны неизго Ата тонит:\n",
            "Нелеском боибр. не клинны зевутсь ехой!,\n",
            "Каз спраны с инко, гулянными,\n",
            "Торь муга васки — с нет погадих,\n",
            "Коль поеверель угкралащи пноса\n",
            "Туть рюфе злаки,\n",
            "В моешах, \n",
            "Затятся ни мни, долькой ни очала:\n",
            "Дели я глебу навенье\n",
            "Я поклясь нипещеннию,Восчий мололи,\n",
            "Нивйт порнылый, \n",
            "Там\n",
            "Достали, бере сог Илил,\n",
            "Вдр, зиблея,\n",
            "Впоею вызны притды -\n",
            "ПототайствиКал.\n",
            "\n",
            "Зайный бурем пойзым\n",
            "К слодьстный лизал безпровели кал?\n",
            "На зноча, Мати резльныла врам чте\n",
            "\tУчиют вбуствиг, не тахихон;\n",
            "И не солнже менял извезю мой\n",
            "Ца зорится талой минлись кромой!\n",
            "Волную и взгрях сяла,\n",
            "Прикалиском делеваньи дерь в мотвойных,\n",
            "На зеребка ликару давец, лиро —\n",
            "Сабо визейнит пуской, друга хыдаловая,\n",
            "Спленной. \n",
            "Не глажу не суди эТут и сладной:\n",
            "\n",
            "Сейлук и ма\n",
            "Иетила пажеть зелась смечтли прясныть в Вессень, и от челье,\n",
            "Но вмай гобмира нам буухачен,\n",
            "Одна бужет, \n",
            "Гроле стал янны сревствтом лучений!\n",
            "Я же безбрюгает, роккая,\n",
            "И в что на с стловованьешь;\n",
            "Он очи блютила враны стразяю\n",
            "Я всоры\n",
            "С пориет, берего в - Он мечтый с сердца!\n",
            "Как скрывых шупской\n",
            "Отда Дубриво:\n",
            "Угет что страть любвыи, горою скалась:\n",
            "Я неть по жль, рмижку и не вотцами\n",
            "\tЯ верню сипу, на зим бавел безднимый лукой,\n",
            "Восночами плажбен красталыся,\n",
            "Чтоб тто от иной\n",
            "Тделиной рветилы, сольцы сон\n",
            "Вчто мнолице\n",
            "Цветонниченный Неселыл на горей негом,\n",
            "Любит тветое,\n",
            "\"Лишист неетали удбет \n",
            "Как - милалы,\n",
            "Муччист так, тос\n"
          ]
        }
      ],
      "source": [
        "# Example model load\n",
        "state_dict = torch.load('model/checkpoint.pt')\n",
        "print(state_dict.keys())\n",
        "model = BigramLanguageModel()\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "# generate\n",
        "context = torch.zeros((1, 1), dtype=torch.long)\n",
        "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
